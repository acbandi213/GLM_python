{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.preprocessing import SplineTransformer, QuantileTransformer\n",
    "from scipy import stats\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "from itertools import accumulate\n",
    "from collections import Counter\n",
    "\n",
    "import encoding_GLM as glm #Import the encoding_GLM package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "#CPU and GPU checks:\n",
    "\n",
    "print(tf.config.experimental.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_each_CV_fold(train_directory, test_directory, fold):\n",
    "    '''\n",
    "        function to locate and arrange the train and test datasets prior to training the GLM encoding model \n",
    "        \n",
    "        inputs:\n",
    "        train_directory: location of the training datasets \n",
    "        test_directory: location of the testing datasets \n",
    "        fold: cross validation fold for the specified dataset \n",
    "        \n",
    "        outputs:\n",
    "        X_train, Y_train: Variables + neural data for training set \n",
    "        X_test, Y_test: Variables + neural data for testing set \n",
    "        count_of_values: # and position of unique variables for fitting \n",
    "        IDS_for_count_of_values: names of the unique variables for fitting - helps for B-weight analysis \n",
    "    '''\n",
    "    directory_train = train_directory.format(fold)\n",
    "    os.chdir(directory_train)\n",
    "\n",
    "    behav = scipy.io.loadmat('behav_big_matrix.mat')\n",
    "    behav_ids = scipy.io.loadmat('behav_big_matrix_ids.mat')\n",
    "\n",
    "    behav_matrix = behav['behav_big_matrix']\n",
    "    behav_ids_matrix = behav_ids['behav_big_matrix_ids'][0]\n",
    "\n",
    "    response = scipy.io.loadmat('combined_response.mat')\n",
    "    response_matrix = response['combined_response']\n",
    "    response_matrix[response_matrix > 0.05] = 1\n",
    "\n",
    "    X_train = behav_matrix\n",
    "    Y_train = response_matrix\n",
    "    \n",
    "    behav_IDS = []\n",
    "    for trial in list(range(behav_ids['behav_big_matrix_ids'][0].shape[0])):\n",
    "        behav_IDS.append(behav_ids['behav_big_matrix_ids'][0][trial][0])\n",
    "\n",
    "    # Count the occurrences of each element in the list\n",
    "    counter = Counter(behav_IDS)\n",
    "\n",
    "    # Get the unique values\n",
    "    unique_values = list(counter.keys())\n",
    "\n",
    "    # Get the count of each unique value\n",
    "    count_of_values = list(counter.values())\n",
    "\n",
    "    IDS_index = np.array(list(accumulate(count_of_values)))-1\n",
    "    IDS_for_count_of_values = []\n",
    "    for index in IDS_index[0:418]:\n",
    "        IDS_for_count_of_values.append(behav_IDS[index])\n",
    "        \n",
    "    directory_test = test_directory.format(fold)\n",
    "    os.chdir(directory_test)\n",
    "\n",
    "    behav = scipy.io.loadmat('behav_big_matrix.mat')\n",
    "    behav_ids = scipy.io.loadmat('behav_big_matrix_ids.mat')\n",
    "\n",
    "    behav_matrix = behav['behav_big_matrix']\n",
    "    behav_ids_matrix = behav_ids['behav_big_matrix_ids'][0]\n",
    "\n",
    "    response = scipy.io.loadmat('combined_response.mat')\n",
    "    response_matrix = response['combined_response']\n",
    "    response_matrix[response_matrix > 0.05] = 1\n",
    "\n",
    "    X_test = behav_matrix\n",
    "    Y_test = response_matrix\n",
    "    \n",
    "    # Clean up design matrix and z-score along sample dimension\n",
    "    X_train = X_train.T\n",
    "    # Multiply deconvolved activity by 10 to mimic spike number\n",
    "    Y_train = 10 * Y_train.T\n",
    "\n",
    "    X_test = X_test.T\n",
    "    # Multiply deconvolved activity by 10 to mimic spike number\n",
    "    Y_test = 10 * Y_test.T\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test, count_of_values, IDS_for_count_of_values\n",
    "\n",
    "def train_model_on_cv_fold(X_train, Y_train, count_of_values, activation, loss_type, regularization, l1_ratio):\n",
    "    '''\n",
    "        function to train the GLM encoding model for an entire fold on local CPU \n",
    "        \n",
    "        inputs:\n",
    "        X_train, Y_train: Variables + neural data for training set \n",
    "        X_test, Y_test: Variables + neural data for testing set \n",
    "        count_of_values: # and position of unique variables for fitting \n",
    "        activation: {'linear', 'exp', 'sigmoid', 'relu', 'softplus'}, default = 'exp'\n",
    "        loss_type: {'gaussian', 'poisson', 'binominal'}, default = 'poisson'\n",
    "        regularization: {'elastic_net', 'group_lasso'}, default = 'elastic_net'\n",
    "        l1_ratio: L1 ratio for elastic_net regularization (l1_ratio = 1. is Lasso, l1_ratio = 0. is ridge), default = 0.\n",
    "        outputs:\n",
    "        model: trained model results (as tensor matrix)\n",
    "    '''\n",
    "    # Reset keras states\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = glm.GLM(activation = activation, loss_type = loss_type, \n",
    "                    regularization = regularization, lambda_series = 10.0 ** np.linspace(-1, -8, 10), \n",
    "                    l1_ratio = l1_ratio, smooth_strength = 0., \n",
    "                    optimizer = 'adam', learning_rate = 1e-3)\n",
    "    \n",
    "    model.fit(X_train, Y_train, feature_group_size = count_of_values, verbose = False) \n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model_on_cv_fold_GPU(X_train, Y_train, count_of_values, activation, loss_type, regularization, l1_ratio):\n",
    "    '''\n",
    "        function to train the GLM encoding model for an entire fold on local GPU - significantly faster  \n",
    "        \n",
    "        inputs:\n",
    "        X_train, Y_train: Variables + neural data for training set \n",
    "        X_test, Y_test: Variables + neural data for testing set \n",
    "        count_of_values: # and position of unique variables for fitting \n",
    "        activation: {'linear', 'exp', 'sigmoid', 'relu', 'softplus'}, default = 'exp'\n",
    "        loss_type: {'gaussian', 'poisson', 'binominal'}, default = 'poisson'\n",
    "        regularization: {'elastic_net', 'group_lasso'}, default = 'elastic_net'\n",
    "        l1_ratio: L1 ratio for elastic_net regularization (l1_ratio = 1. is Lasso, l1_ratio = 0. is ridge), default = 0.\n",
    "        outputs:\n",
    "        model: trained model results (as tensor matrix)\n",
    "    '''\n",
    "    # Ensure the function runs on GPU\n",
    "    with tf.device('/GPU:0'):\n",
    "        # Reset keras states\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        model = glm.GLM(activation = activation, loss_type = loss_type, \n",
    "                        regularization = regularization, lambda_series = 10.0 ** np.linspace(-1, -8, 10), \n",
    "                        l1_ratio = l1_ratio, smooth_strength = 0., \n",
    "                        optimizer = 'adam', learning_rate = 1e-3)\n",
    "        # Capture the start time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(X_train, Y_train, feature_group_size=count_of_values, verbose=False) \n",
    "        \n",
    "        # Capture the end time\n",
    "        end_time = time.time()\n",
    "\n",
    "    print(f\"Time taken on GPU: {end_time - start_time:.4f} seconds\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters \n",
    "\n",
    "activation, loss_type, regularization, l1_ratio = 'exp', 'binomial', 'elastic_net', .95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model for all folds in a dataset:\n",
    "\n",
    "dataset = \n",
    "data_directory = \"/Volumes/Runyan2/Akhil/Runyan-Piasini-2017-master/Data/{}/state1_model\" \n",
    "save_directory = \"/Volumes/Runyan2/Akhil/Runyan-Piasini-2017-master/Data/{}\"\n",
    "\n",
    "frac_dev_expl_all = []\n",
    "for fold_number in list(range(0,10)): #can change this to fit however many folds you need \n",
    "    train_directory = data_directory + \"/prepost trial cv 73 #{}\".format(fold_number+1)\n",
    "    test_directory = data_directory + \"/prepost trial cv 73 #{}/test\".format(fold_number+1)\n",
    "\n",
    "    for i in tqdm(range(5)):\n",
    "        [X_train, Y_train, X_test, Y_test, count_of_values, IDS_for_count_of_values] = get_data_from_each_CV_fold(train_directory,\n",
    "                                                                                                                    test_directory,\n",
    "                                                                                                                    fold_number+1)\n",
    "    model_trained = train_model_on_cv_fold_GPU(X_train, Y_train, count_of_values, activation, loss_type, regularization, l1_ratio)\n",
    "    model_trained.select_model(X_test, Y_test, min_lambda = 0.0, make_fig = False)\n",
    "    frac_dev_expl, dev_model, dev_null, dev_expl = model_trained.evaluate(X_test, Y_test, make_fig = False)\n",
    "    B_weights = model_trained.selected_w\n",
    "    intercept_weight = model_trained.selected_w0\n",
    "    y_pred = model_trained.predict(X_test)\n",
    "    selec_lambda = model_trained.selected_lambda\n",
    "    \n",
    "    model_data = {\n",
    "        'frac_dev_expl': frac_dev_expl,\n",
    "        'dev_model': dev_model,\n",
    "        'dev_null': dev_null, \n",
    "        'dev_expl': dev_expl, \n",
    "        'B_weights': B_weights,\n",
    "        'intercept_weight': intercept_weight,\n",
    "        'y_pred': y_pred,\n",
    "        'selec_lambda': selec_lambda\n",
    "    }\n",
    "    \n",
    "    os.chdir(train_directory)\n",
    "\n",
    "    with open('model_data_a95_GPU.pkl', 'wb') as file:\n",
    "        pickle.dump(model_data, file)\n",
    "        \n",
    "    frac_dev_expl_all.append(model_data['frac_dev_expl'])\n",
    "\n",
    "frac_dev_expl_all_state1 = np.array(frac_dev_expl_all, dtype=object)\n",
    "\n",
    "save_directory = \"/Volumes/Runyan2/Akhil/Runyan-Piasini-2017-master/Data/{}\".format(dataset)\n",
    "os.chdir(save_directory)\n",
    "np.save('performance_state1_a98_GPU.npy', frac_dev_expl_all_state1)\n",
    "print(dataset + ' - DONE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GLM_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
